---
title: "Unsupervised Learning: In-class Materials"
author: "Tomy Tjandra"
date: "`r format(Sys.Date(), '%B %e, %Y')`"
output:
  html_document:
    number_sections: true
    df_print: paged
    highlight: tango
    theme: cosmo
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: true
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
# clear-up the environment
rm(list = ls())

# chunk options
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  comment = "#>"
)

options(scipen = 999)
```

<style>
body {
text-align: justify}
</style>

# Introduction

## Supervised vs Unsupervised

**Supervised Learning**

- Memiliki target variabel
- Tujuannya untuk membuat model prediksi dengan training data
- Model di-evaluasi dengan testing data

**Unsupervised Learning**

- Tidak memiliki target variabel
- Tujuannya untuk mencari pola dalam data, yang berguna untuk menghasilkan informasi. Digunakan pada tahap Pre-processing maupun Exploratory Data Analysis (EDA)
- Tidak perlu melakukan evaluasi model, karena tidak ada label aktual

## Mindmap

```{r, out.width = "100%", echo = FALSE, fig.align = "center"}
knitr::include_graphics("additional_assets/UL.png")
```

Secara umum, unsupervised learning yang akan kita pelajari terbagi menjadi dua:

1. **Dimensionality Reduction**: mereduksi dimensi dengan tetap mempertahankan informasi sebanyak-banyaknya. Algoritma: Principal Component Analysis (PCA)
2. **Clustering**: pengelompokkan data berdasarkan kemiripan karakteristik data. Algoritma: K-means Clustering

# Dimensionality Reduction

Tujuan dimensionality reduction adalah untuk mereduksi banyaknya variabel (dimensi) pada data dengan tetap mempertahankan informasi sebanyak mungkin. Dimensionality reduction dapat mengatasi masalah high-dimensional data. Kesulitan yang dihadapi pada high-dimensional data:

- Memerlukan waktu dan komputasi yang besar dalam melakukan pemodelan
- Melakukan visualisasi lebih dari tiga dimensi

Istilah:

- **Dimensi**: variabel/kolom. Semakin banyak kolom, maka dimensi semakin tinggi.
- **Informasi**: variance. Semakin besar variance, maka informasi semakin banyak.

💡 Mari kita ulas sedikit terkait materi Practical Statistics yaitu variance:

```{r}
a <- c(1,2,3,4,5,6,7,8,9,10)
b <- c(4,5,5,6,6,4,6,5,4,4)

var(a)
var(b)
```

Tapi, hati-hati nilai variance bergantung pada skala/satuan dari variabel.

```{r}
c <- a*100
c

var(c)
```

## Motivation Example: Image Compression

Pada data gambar, setiap kotak pixel akan menjadi 1 kolom. Foto berukuran 40x40 pixel memiliki 1600 kolom (dimensi). Sekarang mari renungkan, berapa spesifikasi kamera handphone anda? Berapa besar dimensi data yang dihasilkan kamera Anda?

Image compression adalah salah satu penerapan dimensionality reduction. Data gambar direduksi dimensinya namun tetap menghasilkan gambar yang serupa (informasi inti tidak hilang), sehingga gambar lebih mudah diproses. Salah satu algoritma dimensionality reduction adalah **Principal Component Analysis (PCA)**.

```{r, out.width = "60%", echo = FALSE, fig.align = "center"}
knitr::include_graphics("assets/lenna.png")
```

## Principal Component Analysis (PCA)

### Concept

Ide dasar dari PCA adalah untuk membuat sumbu (axis) baru yang dapat menangkap informasi sebesar mungkin. Sumbu baru ini adalah yang dinamakan sebagai **Principal Component (PC)**. Untuk melakukan dimensionality reduction, kita akan memilih beberapa PC untuk dapat merangkum informasi yang dibutuhkan.

💡 Kunjungi link berikut untuk ilustrasi PCA secara visual untuk data 2D dan 3D: https://setosa.io/ev/principal-component-analysis/

```{r, out.width = "80%", echo = FALSE, fig.align = "center"}
knitr::include_graphics("additional_assets/pca.png")
```

**Figure 1A (kiri)**:

- Sumbu: X1 dan X2
- Variance data dijelaskan oleh X1 dan X2
- Dibuatlah sumbu baru untuk menangkap informasi X1 dan X2, yang dinamakan PC1 dan PC2

**Figure 1B (kanan)**:

- Sumbu baru: PC1 dan PC2
- PC1 menangkap variance lebih banyak daripada PC2
- Misalkan PC1 menangkap 90% variance, dan sisanya ditangkap oleh PC2 yaitu 10%

> Hanya dengan menggunakan 1 PC (yaitu PC1), kita dapat mereduksi dimensi data sebesar 50% (dari 2 dimensi menjadi 1 dimensi), namun tetap mempertahankan informasi sebesar 90% informasi data asli.

💡 **Catatan**:

- PC dibuat untuk merangkum sebanyak mungkin informasi data
- Banyaknya PC yang dibentuk = Banyaknya dimensi data asli
- PC1 menangkap variance paling besar, daripada PC2, dan seterusnya
- Antar PC **tidak saling berkorelasi**, karena saling tegak lurus
- Metode PCA cocok untuk data numerik yang **saling berkorelasi**

Di antara kedua data berikut, mana yang lebih cocok untuk dilakukan PCA:

```{r echo=FALSE}
RNGkind(sample.kind = "Rounding")
set.seed(100)

par(mfrow=c(1,2))
x <- runif(100)
plot(x=x, y=runif(100), pch=19, cex=0.5, xlim=c(-0.5, 1.5), ylim=c(-0.5, 1.5), main="Blind Tasting", xlab="wine age", ylab="score")
plot(x=x, y=x+runif(100, -0.1, 0.1), pch=19, cex=0.5, xlim=c(-0.5, 1.5), ylim=c(-0.5, 1.5), main="Logistic Machinery", xlab="temperature", ylab="pressure")
```

**Jawaban**: Logistic Machinery, karena antar variable berkolerasi. Bila dibentuk PC1 dan PC2, maka PC1 akan menangkap variansi yang jauh lebih tinggi daripada PC2, sehingga dengan menggunakan PC1 kita dapat menjelaskan data.

Data Blind Tasting tetap bisa diaplikasikan PCA. Namun hasilnya kurang baik karena PC1 dan PC2 yang dibentuk akan menangkap variansi yang kurang lebih sama besar, sehingga tidak dapat dilakukan dimensionality reduction.

## Workflow PCA

### Business Question

Kita sebagai seorang data scientist diminta untuk menerapkan metode dimensionality reduction terhadap data unit bangunan yang terjual di New York Property Market selama periode 12 bulan. Sumber data diperoleh dari New York City Department of Finance.

### Read data

```{r message=FALSE}
library(dplyr)

property <- read.csv("data_input/nyc.csv")
glimpse(property)
```

Deskripsi data:

* `X`: ID baris
* `BOROUGH`: Kode digit untuk wilayah dimana properti berada; Manhattan (1), Bronx (2), Brooklyn (3), Queens (4), dan Staten Island (5)
* `NEIGHBORHOOD`: Nama lingkungan
* `BUILDING.CLASS.CATEGORY`: Kategori kelas properti
* `TAX.CLASS.AT.PRESENT`, `TAX.CLASS.AT.TIME.OF.SALE`: Jenis pajak bangunan
* `BLOCK`, `LOT`: Kombinasi borough, blok, dan lot membentuk kode unik untuk properti di New York City
* `EASE.MENT`: Ragam hak yang dimiliki bangunan (contoh: hak jalan, dsb.)
* `BUILDING.CLASS.AT.PRESENT`, `BUILDING.CLASS.AT.TIME.OF.SALE`: Kategori kepemilikan bangunan (A: rumah satu keluarga; O: gedung kantor; R: kondominium (kepemilikan bersama); dsb.)
* `ADDRESS`: Alamat properti
* `APARTMENT.NUMBER`: Nomor apartemen
* `ZIP.CODE`: Kode pos properti
* `RESIDENTIAL.UNITS`: Jumlah unit hunian di properti yang terdaftar
* `COMMERCIAL.UNITS`: Jumlah unit komersial di properti yang terdaftar
* `TOTAL.UNITS`: Jumlah unit hunian dan komersial
* `LAND.SQUARE.FEET`: Luas tanah properti (square feet)
* `GROSS.SQUARE.FEET`: Total area semua lantai bangunan yang diukur dari permukaan luar dinding luar bangunan, termasuk area tanah dan ruang di dalam setiap bangunan atau struktur pada properti
* `YEAR.BUILT`: Tahun properti dibangun
* `SALE.PRICE`: Harga properti terjual; bila 0 maka properti merupakan warisan
* `SALE.DATE`: Tanggal properti dijual

### Data Cleansing

1. Buang kolom yang tidak diperlukan:

* Kolom yang tidak relevan: `X`
* Kolom dengan semua nilai NA: `EASE.MENT`

2. Memperbaiki tipe data yang belum tepat:

Perhatikan yang integer:

* `BOROUGH`, `BLOCK`, `LOT`: int -> factor
* `ZIP.CODE`: int -> factor
* `RESIDENTIAL.UNITS`, `COMMERCIAL.UNITS`, `TOTAL.UNITS`: int -> int
* `YEAR.BUILT`: int -> int (tergantung kebutuhan, dalam kasus ini misal kita ingin mengetahui bagaimana year.built berkorelasi dengan sale.price)
* `TAX.CLASS.AT.TIME.OF.SALE`: int -> factor

Perhatikan yang character:

* `NEIGHBORHOOD`: chr -> factor
* `BUILDING.CLASS.CATEGORY`, `BUILDING.CLASS.AT.PRESENT`, `BUILDING.CLASS.AT.TIME.OF.SALE`: chr -> factor
* `TAX.CLASS.AT.PRESENT`: chr -> factor
* `ADDRESS`: chr -> chr
* `APARTMENT.NUMBER`: chr -> chr
* `LAND.SQUARE.FEET`, `GROSS.SQUARE.FEET`: chr -> int
* `SALE.PRICE`: chr -> int
* `SALE.DATE`: chr -> date

```{r warning=FALSE}
property_clean <- property %>% 
  select(-c(X, EASE.MENT)) %>% # drop kolom
  mutate_at(vars(BOROUGH, BLOCK, LOT, ZIP.CODE, TAX.CLASS.AT.TIME.OF.SALE), as.factor) %>% 
  mutate_at(vars(LAND.SQUARE.FEET, GROSS.SQUARE.FEET, SALE.PRICE), as.integer) %>% 
  mutate(SALE.DATE = lubridate::ymd_hms(SALE.DATE)) %>% 
  mutate_if(is.character, as.factor) %>% # mengubah 6 kolom chr -> factor
  mutate_at(vars(ADDRESS, APARTMENT.NUMBER), as.character) # kembalikan jadi chr

head(property_clean)
```

Cek missing values:

```{r}
colSums(is.na(property_clean))
```

Kolom dengan missing values: `LAND.SQUARE.FEET`, `GROSS.SQUARE.FEET`, `SALE.PRICE`

Beberapa cara untuk handle missing values:

1. Imputasi: mengisi nilai NA dengan suatu nilai
2. Drop column: hapus kolom apabila NA pada kolom tersebut terlalu banyak (di atas 5%)
3. Drop row: hapus baris apabila mengandung nilai NA

**Catatan**: Fokus pada course ini bukan di handle missing values, namun untuk memahami PCA dan visualisasinya, sehingga mari kita gunakan cara yang cukup simpel, yaitu cara ketiga. Dengan justifikasi bahwa kita tetap membutuhkan semua kolom tersebut dalam analisis kita. 

```{r warning=FALSE}
# drop baris yang mengandung NA
property_clean <- property_clean %>% 
  filter(complete.cases(.)) # ambil baris yang komplit saja
```

Note: `.` pada `complete.cases()` mengacu kepada object `property_clean`

Dikarenakan analisis PCA menggunakan nilai **variance**, kita hanya dapat mereduksi kolom bertipe data numerik.

```{r}
# data untuk PCA
ppt <- property_clean %>% 
  select_if(is.numeric) # hanya mengambil kolom bertipe data numerik

glimpse(ppt)
```

### Exploratory Data Analysis

Apakah antar variable memiliki skala yang sama? Mari kita cek range untuk tiap kolom:

```{r}
summary(ppt)
```

Mari kita cek juga matriks covariance dari `ppt`:

```{r}
var(ppt) # atau cov(ppt) juga bisa
```

Variance dari masing-masing variabel berbeda jauh karena range/skala dari tiap variabel berbeda, begitupun covariance. **Nilai variance dan covariance dipengaruhi oleh skala dari data**. Semakin tinggi skala, nilai variance atau covariance akan semakin tinggi.

⚠️ Data dengan perbedaan skala antar variabel yang tinggi **tidak baik untuk langsung dianalisis PCA karena dapat menimbulkan bias**. PC1 akan dianggap menangkap variansi tertinggi dan PC selanjutnya dianggap tidak memberikan informasi.

Perhatikan plot di bawah ini bahwa semua informasi hanya dirangkum oleh PC1. Hal ini terjadi karena `SALE.PRICE` memiliki skala hingga milyaran (variansinya tinggi) dibandingkan variabel lainnya. 

```{r echo=FALSE}
# melihat variance yang dirangkum tiap PC
# fungsi prcomp() akan dijelaskan saat masuk bagian PCA
plot(prcomp(ppt))
```

💡 Oleh sebab itu, harus kita scaling terlebih dahulu sebelum melakukan PCA.

### Data Preprocessing: Scaling

Scaling dilakukan agar antar variabel memiliki skala yang tidak jauh berbeda. Nilai scaling yang digunakan adalah Z-score (mean = 0, standar deviasi = 1)

$$Z = \frac{x-mean}{sd}$$

```{r}
# scaling
ppt_z <- scale(ppt)
summary(ppt_z)
```

```{r}
# melihat variance yang dirangkum tiap PC
plot(prcomp(ppt_z))
```

Sekarang, PC1 sudah tidak mendominasi PC lainnya dalam besarnya variance yang ditangkap.

### Principal Component Analysis using `prcomp()`

PCA dapat dilakukan dengan menggunakan fungsi `prcomp()`.

**Cara 1**: menggunakan data yang di-scale secara terpisah, yaitu `ppt_z`

```{r}
# menggunakan data yang sudah di-scaling
prcomp(ppt_z)
```

**Cara 2**: menggunakan data yang belum discale, lalu tambahkan parameter `scale = T` 

```{r}
pca <- prcomp(ppt, scale = T)
pca
```

Terdapat tiga komponen utama dalam objek `pca`:

1. `pca$sdev`: standar deviasi (akar variance) yang ditangkap oleh masing-masing PC, disebut juga sebagai **eigen value**. Digunakan untuk mengetahui seberapa **besaran informasi** yang ditangkap masing-masing PC.

```{r}
pca$sdev^2
```

```{r}
# variance yang ditampilkan dalam bentuk plot
plot(pca)
```

2. `pca$rotation`: matrix rotasi yang berfungsi sebagai **formula** untuk memproyeksikan titik ke masing-masing PC, matrix ini terdiri dari **eigen vectors**. Digunakan untuk mengetahui **kontribusi** masing-masing variabel ke PC.

```{r}
data.frame(pca$rotation)
```

Formula mirip seperti pada regresi linear, namun tanpa intercept:

PC1 = -0.501 * RESIDENTIAL.UNITS - 0.185 * COMMERCIAL.UNITS + ... - 0.226 * SALE.PRICE
PC2 = 0.089 * RESIDENTIAL.UNITS - 0.816 * COMMERCIAL.UNITS + ... + 0.222 * SALE.PRICE
PC3 = -0.019 * RESIDENTIAL.UNITS + 0.015 * COMMERCIAL.UNITS + ... + 0.070 * SALE.PRICE

dst sampai PC7

3. `pca$x`: nilai hasil proyeksi titik ke PC untuk tiap baris. Digunakan untuk mendapatkan nilai data yang baru.

```{r}
data.frame(head(pca$x))
```

### Reduce Dimension

Misal kita hanya ingin mereduksi 7 dimensi menjadi 5 dimensi, maka kita gunakan kolom PC1 sampai PC5 pada `pca$x`.

Namun bagaimana kita memilih berapa dimensi yang kita pertahankan? Kita melihat **cumulative variance** untuk melakukan dimensionality reduction dengan fungsi `summary()`.

```{r}
summary(pca)
```

- **Standard deviation**: standar deviasi (akar variance) yang ditangkap oleh masing-masing PC, sama seperti `pca$sdev`
- **Proportion of Variance**: informasi yang ditangkap oleh tiap PC
- **Cumulative Proportion**: jumlah informasi yang ditangkap secara kumulatif dari PC1 hingga PC tersebut

Pemilihan banyaknya PC disesuaikan dengan kebutuhan informasi. Misal, bila kita ingin merangkum minimal 75% informasi, maka jumlah PC yang kita gunakan adalah **PC 1 - 4**

```{r}
# subsetting: baris, kolom
# mengambil 4 kolom pertama
head(pca$x[,1:4])
```

```{r}
pc_keep <- data.frame(pca$x[,1:4])
head(pc_keep)
```

Setelah dipilih PC yang merangkum informasi yang dibutuhkan, PC dapat digabung dengan data awal dan digunakan untuk analisis lebih lanjut (misal: supervised learning).

```{r}
property_clean %>% 
  select_if(purrr::negate(is.integer)) %>% # ambil kolom selain integer
  cbind(pc_keep) # gabungkan dengan PC
```

## Pros and Cons

Kelebihan melakukan PCA sebelum modeling:

+ Beban komputasi apabila dilakukan pemodelan relatif lebih rendah
+ Bisa jadi salah satu teknik untuk improve model, namun tidak selalu menjadi lebih baik
+ Mengurangi resiko terjadinya multikolinearitas, karena nilai antar PC sudah tidak saling berkorelasi

```{r}
library(GGally)
# korelasi sebelum PCA
ggcorr(ppt, label = T)

# korelasi setelah PCA
ggcorr(pca$x, label = T)
```

Kekurangan melakukan PCA sebelum modeling:

- Model sulit untuk diinterpretasikan karena nilai PC merupakan campuran dari beberapa variable.

## [Optional] Mathematics of PCA

💡 Slide: https://docs.google.com/presentation/d/1jXmwtKdpv3EjLymhkq1OzyUUJBcrA3q2u_vqlYv_feQ/edit#slide=id.gb7882dd136_0_1375

💡 Video: [3Blue1Brown: Eigenvectors and eigenvalues](https://www.youtube.com/watch?v=PFDu9oVAE-g)

Untuk membentuk PC dan nilai pada PC dibutuhkan **eigen values** & **eigen vector**. Secara manual, eigen values dan eigen vector didapatkan dari operasi matrix.

Teori matrix:

* skalar: nilai yang memiliki magnitude/besaran
* vektor: nilai yang memiliki besaran dan arah (umum digambarkan dalam suatu koordinat)
* matrix: kumpulan nilai/bentukan data dalam baris dan kolom

```{r, out.width = "60%", echo = FALSE, fig.align = "center"}
knitr::include_graphics("additional_assets/ul12.png")
```

**Perkalian skalar-vektor**: mengubah besaran vektor (tidak merubah arah kecuali berbalik arah)

$$2\left(\begin{array}{cc}2\\3\end{array}\right) = \left(\begin{array}{cc}4\\6\end{array}\right)$$
$$-2\left(\begin{array}{cc}2\\3\end{array}\right) = \left(\begin{array}{cc}-4\\-6\end{array}\right)$$

**Perkalian matrix-vektor**: mengubah besaran dan arah

$$\left(\begin{array}{cc}1 & 2\\-1 & 2\end{array}\right)\left(\begin{array}{cc}2\\3 \end{array}\right) = \left(\begin{array}{cc}8\\4\end{array}\right)$$ 

**Perkalian Matrix-Vektor Spesial**

* **Matrix Rotasi**: mengubah arah vektor tanpa mengubah besaran

$$
\left(\begin{array}{cc} 
0 & -1\\ 
1 & 0 
\end{array}\right)
\left(\begin{array}{cc} 
2\\ 
3
\end{array}\right)=
\left(\begin{array}{cc} 
-3\\ 
2
\end{array}\right)
$$

```{r}
# perkalian matrix di R
matrix(c(0,1,-1,0), nrow=2) %*% as.vector(c(2,3))
```

* **Matrix Identitas**: tidak mengubah besaran maupun arah vektor

$$
\left(\begin{array}{cc} 
1 & 0\\ 
0 & 1 
\end{array}\right)
\left(\begin{array}{cc} 
2\\ 
3
\end{array}\right)=
\left(\begin{array}{cc} 
2\\ 
3
\end{array}\right)
$$

```{r}
# perkalian matrix di R
matrix(c(1,0,0,1), nrow=2) %*% as.vector(c(2,3))
```

**Eigen dari suatu Matrix**

Untuk setiap matrix, terdapat vektor spesial (eigen vector) yang jika dikalikan dengan matrixnya, hasilnya akan sama dengan vektor tersebut dikalikan suatu skalar (eigen value). Sehingga didapatkan rumus:

$$Ax = \lambda x$$
Dimana $x$ adalah eigen vector dan $\lambda$ adalah eigen value dari matrix $A$.

$$
\left(\begin{array}{cc} 
2 & 3\\ 
2 & 1 
\end{array}\right)
\left(\begin{array}{cc} 
3\\ 
2
\end{array}\right)
=
\left(\begin{array}{cc} 
12\\ 
8
\end{array}\right)
=4
\left(\begin{array}{cc} 
3\\ 
2
\end{array}\right)
$$

Teori eigen dipakai untuk menentukan PC dan nilai-nilai pada PC.

**Penerapan Eigen dalam PCA:**

**Matrix covariance** adalah matrix yang dapat merangkum informasi (variance) dari data. Kita menggunakan matrix covariance untuk mendapatkan mendapatkan eigen vector dan eigen value dari matrix tersebut, dimana:

* **eigen vector**: arah sumbu tiap PC, yang menjadi formula untuk mentransformasi data awal ke PC baru. 
* **eigen value**: variansi yang ditangkap oleh setiap PC.
* tiap PC memiliki 1 eigen value & 1 eigen vector.
* alur: matrix covariance -> eigen value -> eigen vector -> nilai di tiap PC

Eigen vector akan menjadi formula untuk kalkulasi nilai di setiap PC. Contohnya, untuk data yang terdiri dari 2 variabel, bila diketahui eigen vector dari PC1 adalah:

$$x_{PC1}= \left(\begin{array}{cc}b_1\\b_2\end{array}\right)$$

Maka formula untuk menghitung nilai pada PC1 (untuk tiap barisnya) adalah:

$$PC1= b_1X1 + b_2X2$$

Keterangan:

* $x_{PC1}$ : eigen vector PC1 dari matrix covariance
* $b_1$, $b_2$ : konstanta dari eigen vector
* $PC1$ : nilai di PC1
* $X1$, $X2$ : nilai variabel X1 dan X2 di data awal

**Mencari Eigen dengan R**

```{r fig.height=5, fig.width=5}
# membuat data dummy
RNGkind(sample.kind = "Rounding")
set.seed(100)

x1 <- runif(200) # buat data random dengan distribusi normal
data <- data.frame(x1=x1, x2=-x1+runif(100, 1.05, 1.25)) # buat data frame
data <- scale(data) # agar skala antar variable sama
plot(data)
```

```{r}
# matrix covariance
A <- cov(data)
A
```

```{r}
# menghitung eigen
eigen(A)
```

**Keterangan**:

* `eigen(A)$values`: Eigen value untuk tiap PC, besar variansi yang dapat ditangkap oleh tiap PC. Eigen value tertinggi adalah milik PC1, kedua tertinggi milik PC2, dan seterusnya. 

* `eigen(A)$vectors`: Eigen vector untuk tiap PC. Eigen vector untuk PC1 adalah kolom pertama `[,1]`; untuk PC2 adalah kolom kedua `[,2]`.

## 📝 SUMMARY DAY 1

**Supervised vs Unsupervised**

- Supervised: memiliki target variable, untuk prediksi
- Unsupervised: tidak memiliki target variable, untuk memahami pola data

**Dimensionality Reduction dengan PCA**

* Istilah:

- Dimensi = banyaknya kolom/variabel
- Informasi = variance

* Konsep:

- Tujuan dimensionality reduction: mereduksi dimensi dengan tetap mempertahankan informasi sebesar mungkin
- PCA merangkum variance dari data awal dengan cara membuat sumbu baru (Principal Component), di mana informasi paling besar ditangkap oleh PC1, diikuti dengan PC2, dst
- Banyaknya PC yang dibuat adalah sebanyak jumlah dimensi pada data awal
- PCA baik digunakan untuk tipe data numerik yang saling berkorelasi, dan akan menghasilkan nilai PC yang saling tidak berkorelasi

**Workflow PCA**

1. Business question
2. Read data
3. Data cleansing: menyesuaikan tipe data, cek dan handle NA
4. EDA: cek satuan data
5. Preprocessing: scaling apabila satuan berbeda
6. PCA dengan `prcomp()`:
  - `pca$sdev` adalah akar kuadrat dari variance yang dirangkum tiap PC
  - `pca$rotation` adalah formula untuk menghasilkan nilai PC
  - `pca$x` adalah nilai di tiap PC untuk tiap observasi
7. Reduksi dimensi: menentukan berapa PC yang sebaiknya digunakan dengan melihat cumulative proportion pada `summary(prcomp())`
8. Pemilihan PC dapat digunakan untuk analisis selanjutnya, misal untuk supervised learning ataupun visualisasi

<center>**<----------- END OF DAY 1 ----------->**</center>

**Tambahan istilah:**

- Eigen value = variance (informasi) yang ditangkap oleh tiap PC
- Eigen vector = formula untuk mengubah data asli menjadi nilai PC (sumbu baru)

***

## Visualizing PCA

PCA tidak hanya berguna untuk dimensionality reduction namun baik untuk visualisasi high-dimensional data. Visualisasi dapat menggunakan **biplot** yang menampilkan:

1. **Individual factor map**, yaitu sebaran data secara keseluruhan menggunakan 2 PC. Tujuannya untuk mengetahui observasi yang serupa dan outlier.
2. **Variables factor map**, yaitu plot yang menunjukkan korelasi antar variable dan kontribusinya terhadap PC.

### Biplot

Mari kita buat biplot dari 100 observasi pertama dari `ppt`

Parameter fungsi `biplot()`:

- `x`: objek hasil PCA
- `cex`: ukuran font di biplot

```{r}
ppt_100 <- ppt %>% 
  head(100)

# melakukan PCA
ppt_100_pca <- prcomp(ppt_100, scale = T)
  
# membuat biplot
biplot(ppt_100_pca,
       cex = 0.5, # mengatur font size pada biplot
       scale = F) # agar nilai yang ditampilkan sesuai dengan nilai pada $rotation yang sebenarnya
```


Biplot menggunakan **PC1** dan **PC2** secara default, karena PC tersebut yang merangkum informasi terbanyak dari data sehingga mampu mewakili data kita.

- Titik-titik = index angka dari **observasi**. Semakin berdekatan maka karakteristiknya semakin mirip, sedangkan yang jauh dari gerombolan data dianggap sebagai outlier
- Panah merah = **loading score**, menunjukkan kontribusi variabel tersebut terhadap PC, atau banyaknya informasi variabel tersebut yang dirangkum oleh PC.

#### Variable

1. **Variable Contribution**

Loading score dilihat dari jarak titik nol (pusat) ke ujung panah, secara horizontal (PC1) / vertikal (PC2). Semakin jauh panah, semakin banyak informasi yang dirangkum. Bila panah kearah negatif, maka bila nilai variabel awal tinggi, nilai di PC akan rendah, dan sebaliknya.

Berdasarkan penjelasan tersebut, maka:

- Variable yang paling berkontribusi ke PC1 adalah: `GROSS.SQUARE.FEET`, `LAND.SQUARE.FEET`, `SALE.PRICE`
- Variable yang paling berkontribusi ke PC2 adalah: `TOTAL.UNITS`, `RESIDENTIAL.UNITS`

Dari panah merah tersebut, kita tahu variable mana yang paling banyak berkontribusi untuk tiap PC. Namun kita kesulitan untuk mengurutkan kontribusinya, mari kita gunakan fungsi `fviz_contrib()` untuk melihat urutan **kontribusi variabel ke tiap PC**

Parameter:

- `X`: objek PCA
- `choice = "var"`: VARiable contribution
- `axes`: PC keberapa yang ingin kita lihat variable contributionnya

```{r}
# install.packages("factoextra")
library(factoextra)

fviz_contrib(ppt_100_pca,
             choice = "var", # kontribusi variable
             axes = 2) # PC ke berapa?
```

**Catatan**: garis putus-putus merah adalah batas nilai kontribusi yang diharapkan apabila PC menangkap informasi dari tiap variable dengan sama rata. Cara menghitungnya adalah 100%/jumlah variable awal. Pada kasus ini 100%/7 = 14.28%. Bila melebihi batas tersebut, maka dapat dikatakan variable tersebut berkontribusi terhadap PC tersebut.

2. **Korelasi antar variabel** dapat dilihat dari sudut antar dua panah 

- Panah saling berdekatan (sudut antar panah < 90), maka korelasi positif
- Panah saling tegak lurus (sudut antar panah = 90), maka tidak berkorelasi
- Panah saling bertolak belakang (sudut antar panah mendekati 180), maka korelasi negatif

Panah yang berhimpit pada biplot adalah: `GROSS.SQUARE.FEET` dengan `LAND.SQUARE.FEET` (korelasi positif)

```{r}
# opsional: untuk cek korelasi
cor(ppt_100$GROSS.SQUARE.FEET, ppt_100$LAND.SQUARE.FEET)
```

Panah yang (hampir) tegak lurus adalah: `TOTAL.UNITS` dengan `SALE.PRICE` (hampir tidak ada korelasi)

```{r}
cor(ppt_100$TOTAL.UNITS, ppt_100$SALE.PRICE)
```

**Pertanyaan**: Apakah cek korelasi boleh langsung ke data asli? Karena di PCA kan ada scaling?

**Jawaban**: Scaling z-score tidak akan mengubah nilai korelasi, buktinya seperti di bawah ini:

```{r}
ppt_100_z <- as.data.frame(scale(ppt_100))
cor(ppt_100_z$TOTAL.UNITS, ppt_100_z$SALE.PRICE)
```

#### Individual

1. **Outlier detection**: observasi yang jauh dari kumpulan observasi lainnya mengindikasikan outlier dari keseluruhan data. Observasi ini dapat ditandai untuk nantinya dicek karakteristik datanya untuk keperluan bisnis, atau apakah mempengaruhi performa model/clustering, dll.

> Dari biplot di atas, tiga outlier terluar adalah observasi 51, 32, 96

2. **Observasi searah panah** mengindikasikan observasi tersebut nilainya tinggi pada variabel tersebut. Bila bertolak belakang, maka nilainya rendah pada variable tersebut.

Tiga outlier tersebut tinggi di nilai apa saja?

- Observasi 51 ekstrim di nilai sale price, gross, land square feet
- Observasi 96 ekstrim di nilai total units
- Observasi 32 ekstrim di nilai gross, land square feet dan total units

Bandingkan observasi ke 82 dengan 96 dengan biplot diatas, insight apa yang dapat diambil?

```{r}
range(ppt_100$TOTAL.UNITS)
```

```{r}
# cek observasi 82 dan 96
ppt_100[c(82, 96),]
```

- Observasi 82 memiliki nilai `TOTAL.UNITS` yang ekstrim kecil, karena terletak di belakang panah
- Observasi 96 memiliki nilai `TOTAL.UNITS` yang ekstrim besar, karena terletak searah panah

### Dive Deeper I: Biplot `property`

Buatlah biplot dengan menggunakan 300 data pertama dari `ppt`:

```{r}
# your code here
ppt_300 <- head(ppt, n = 300)

# melakukan PCA
ppt_300_pca <- prcomp(ppt_300, scale = T) # scaling z-score
  
# membuat biplot
biplot(ppt_300_pca,
       cex = 0.5,
       scale = F) # agar range sumbu yang ditampilkan pada biplot sesuai dengan nilai pada object pca
```

**Insight: Outlier**

Dari biplot, identifikasilah:

- Tiga observasi yang memiliki nilai residential dan total unit yang sangat tinggi: **110, 114, 115**
- Tiga observasi yang memiliki nilai jual (SALE.PRICE) yang sangat tinggi: **118, 116, 103**

**Insight: Variable Contribution**

1. Tiga variabel yang berkontribusi tinggi ke PC1: gross square feet, land square feet, dan sale price
2. Dua variabel yang berkontribusi tinggi ke PC2: residential dan total unit
3. Variabel yang berkontribusi rendah pada PC1 maupun PC2: commercial unit dan year built

Untuk validasi insight di atas, visualisasikan kontribusi variabel tiap PC menggunakan `fviz_contrib()` dari library `factoextra`:

```{r}
# your code here
library(factoextra)

# PC1
fviz_contrib(ppt_300_pca, choice = "var", axes = 1)

# PC2
fviz_contrib(ppt_300_pca, choice = "var", axes = 2)
```

**Insight: Variable Correlation**

Gunakan biplot, identifikasilah:

1. Pasangan variabel yang berkorelasi tinggi positif (berhimpit):
  - gross, land square feet, dan sale price
  - residential dan total unit

2. Pasangan variabel yang tidak berkorelasi (tegak lurus):
  - total unit dan sale price
  
```{r}
# opsional: validasi korelasi dengan ggcorr
library(GGally)
ggcorr(ppt_300, label=T)
```

### Dive Deeper II: Biplot `USArrests`

Lakukan PCA untuk built-in dataset `USArrests` dan visualisasikan menggunakan `fancy_biplot()`!

```{r}
# get function & data
source("R/biplot.R")

data("USArrests")
head(USArrests)
```

Dataset `USArrests` merupakan data tentang jumlah tiga tindak kejahatan (`Murder`, `Assault`, dan `Rape`) per 100 ribu penduduk yang berhasil dicatat di 50 negara bagian US pada tahun 1973.

Deskripsi data:

- `Murder`: murder arrests per 100,000
- `Assault`: assault arrests per 100,000
- `UrbanPop`: percent of population in urban areas
- `Rape`: rape arrests per 100,000

```{r}
# PCA & Visualization
fancy_biplot(prcomp(USArrests, scale = T))
```

Insight dari biplot:

1. Perhatikan cluster di mana Arizona, Colorado, Illinois, dan Texas berada. Negara bagian mana lagi yang ada dalam cluster tersebut? **New York, dan bisa disimpulkan kelima negara bagian ini tingkat Rapenya cukup tinggi**

2. Di antara Ohio, Louisiana, North Dakota, dan Colorado, manakah yang memiliki:

- Tingkat `Murder` tertinggi: **Louisiana**
- Tingkat `Assault` tertinggi: **Louisiana**
- Tingkat `Rape` tertinggi: **Colorado**
- Tingkat kriminalitas terendah, meliputi ketiga tipe kejahatan: **North Dakota**

## PCA using `FactoMineR` package

`FactoMineR` adalah R package yang dibuat untuk *exploratory multivariate data analysis*. Mari kita lakukan analisis PCA untuk Dataset Loan dari sebuah platform di quarter ke-4 tahun 2017. 

### Read Data

```{r}
loan <- read.csv("data_input/loan2017Q4.csv",
                 stringsAsFactors = T)
glimpse(loan)
```

Data berisikan 1556 customer dengan 16 variabel, dengan kolom `not_paid` yang mengindikasikan apakah customer tersebut gagal bayar pinjaman atau tidak. Berikut adalah deskripsi data yang lengkap:

- `initial_list_status`: Either w (whole) or f (fractional). This variable indicates if the loan was a whole loan or fractional loan. For background: Some institutional investors have a preference to purchase loans in their entirety to obtain legal and accounting treatment specific to their situation - with the added benefit of “instant funding” to borrowers
- `purpose`: Simplified from the original data; One of: credit_card, debt_consolidation, home_improvement, major_purchase and small_business
- `int_rate`: Interest rate in percentages
- `installment`: Monthly payment owed by the borrower
- `annual_inc`: Self-reported annual income provided by the borrower / co-borrowers during application
- `dti`: A ratio of the borrower’s total monthly debt payments on his/her total obligations to the self-reported monthly income (debt to income ratio)
- `verification_status`: is the reported income verified, not verified, or if the income source was verified
- `grade`: software-assigned loan grade
- `revol_bal`: total credit revolving balance (in the case of credit card, it refers to the portion of credit card spending that goes unpaid at the end of a billing cycle)
- `inq_last_12m`: number of credit inquiries in the last 12 months
- `delinq_2yrs`: number of 30+ days past-due incidences of delinquency in the borrower’s credit file for the past 2 years
- `home_ownership`: one of MORTGAGE, OWN and RENT
- `not_paid`: 1 for charged-off, past-due / grace period or defaulted, 0 for fully-paid loans
- `log_inc`: log of annual_inc
- `verified`: 0 for “Not verified” under verification_status, 1 otherwise
- `grdCtoA`: 0 for a grade of A, B or C, 1 otherwise

### Data Cleansing

Adakah tipe data yang belum tepat?

- ubah `not_paid`, `verified`, `grdCtoA` dari int menjadi factor

```{r}
loan_clean <- loan %>% 
  mutate_at(vars(not_paid, verified, grdCtoA), as.factor)
  
glimpse(loan_clean)
```

### PCA

`FactoMineR` menyediakan dua fungsi utama dalam PCA:

1. `PCA()` untuk melakukan analisis PCA
2. `plot.PCA()` untuk visualisasi

💡 Kelebihan `FactoMineR` dari fungsi base `biplot()` adalah:

- memisahkan dua grafik yang terdapat pada biplot yaitu individual factor map dan variables factor map
- dapat melibatkan variable kategorik untuk mewarnai plot sehingga plot lebih informatif

```{r}
glimpse(loan_clean)
```

```{r}
# indeks kolom numerik (quantitative)
quantivar <- c(3:6,9:11,14)

# indeks kolom kategorik (qualitative)
qualivar <- c(1,2,7,8,12,13,15,16)
```

Sebagai alternatif, apabila tidak ingin manual:

Code dibawah ini untuk memilih indeks kolom numerik dan kategorik secara dinamis (agar lebih scalable untuk kolom yang jumlahnya sudah banyak)

```{r}
# bagian ini opsional
# nama kolom numerik (quantitative)
quanti <- loan_clean %>% 
  select_if(is.numeric) %>% 
  colnames()

# indeks kolom numerik
quantivar <- which(colnames(loan_clean) %in% quanti)

# nama kolom kategorik (qualitative)
quali <- loan_clean %>% 
  select_if(is.factor) %>% 
  colnames()

# indeks kolom kategorik
qualivar <- which(colnames(loan_clean) %in% quali)
```

**Melakukan PCA dengan library `FactoMineR`**

Parameter fungsi `PCA()`:

- `X`: dataframe awal
- `scale.unit = T`: untuk melakukan scaling
- `quali.sup`: index kolom variable kategorikal (qualitative)
- `graph = F`: untuk tidak menampilkan plot secara langsung
- `ncp`: banyaknya PC yang digunakan, default = 5

```{r}
loan_clean %>% 
  select_if(is.numeric)
```


```{r}
# install.packages("FactorMineR")
library(FactoMineR)

# melakukan PCA dengan FactoMineR
loan_pca <- PCA(X = loan_clean, # data awal
                scale.unit = T, # scaling
                quali.sup = qualivar, # index kolom qualitative
                graph = F, 
                ncp = 8) # 8 kolom numerik pada loan_clean

# untuk melihat cumulative variance
# ekuivalen dengan summary(pca) saat menggunakan prcomp()
loan_pca$eig
```

- ind = individual
- coord = coordinate

```{r}
# cek nilai di tiap PC, ekuivalen dengan pca$x
head(loan_pca$ind$coord)
```

### Visualisasi PCA

Biplot dapat divisualisasikan menggunakan `plot.PCA()`

#### Individual Factor Map

Plot sebaran observasi untuk mengetahui index yang dianggap sebagai outlier

Parameter fungsi `plot.PCA()`:

- `x`: object PCA (`FactoMineR`)
- `choix = "ind"`: plot INDividual factor map
- `invisible = "quali"`: menghilangkan label kolom kategorik, karena mengganggu visual
- `select = "contrib n"`: menampilkan indeks dari $n$ outlier terluar
- `habillage`: mewarnai titik berdasarkan index kolom (bisa kategorikal maupun numerik)

```{r}
glimpse(loan_clean)
```

```{r}
plot.PCA(x = loan_pca,
         choix = "ind", # individual factor map (sebaran data)
         invisible = "quali",
         select = "contrib 10", # melabelkan 10 outlier terluar
         habillage = 13 # mewarnai scatter plot berdasarkan kolom ke-13 yaitu not_paid
         )
```

Sepuluh outlier terluar: 1228, 1310, 1438, 43, 351, 1432, 1146, 512, 368, 749

Tindak lanjut terhadap outlier ini:

- Identifikasi nilai apa yang menyebabkan observasi tersebut disebut outlier, lihat dari variable factor map (panah searah dengan observasi)
- Apabila masuk ke ranah supervised learning, outlier ini bisa dicoba untuk dihandle (misal remove/transformasi nilai) sebagai salah satu strategi model improvement

#### Variables Factor Map

- Mengetahui variable contribution ke tiap PC, serta besar informasi yang dirangkum dari tiap variable ke tiap PC (loading)
- Mengetahui correlation antar variable awal

Parameter fungsi `plot.PCA()`:

- `x`: object PCA (`FactoMineR`)
- `choix = "var"`: plot VARiable factor map

```{r}
plot.PCA(x = loan_pca,
         choix = "var")
```

**Note**: persentase yang tampil pada sumbu Dim 1 (29.37%) dan Dim 2 (17.56%) menunjukkan seberapa besar sumbu merangkum informasi. Secara kolektif, biplot di atas menjelaskan 29.37% + 17.56% = 46.93% informasi data asli

**Insight**:

- PC1 paling banyak merangkum dua variable: annual_inc dan log_inc
- PC2 paling banyak merangkum dua variable: int_rate dan dti
- Pasangan variabel yang saling berkorelasi tinggi positif:
  - annual_inc dan log_inc
  - installment dan revol_bal
  - int_rate dan dti

### 📝 SUMMARY DAY 2

**Biplot**

* PCA dapat digunakan untuk visualisasi high-dimensional data, dinamakan biplot:

1. Individual Factor Map: plot persebaran data, tujuannya untuk:

  - Outlier detection (observasinya jauh dari gerombolan data)
  - Mengetahui observasi mana saja yang mirip (yang observasinya membentuk gerombolan data)

2. Variables Factor Map: 

  - Kontribusi tiap variable ke PCnya, panjangnya menggambarkan besar kontribusi
  - Dilihat dari sudut antar panah, kita dapat mengetahui korelasi antar variable:
    + < 90 derajat = korelasi positif
    + 90 derajat = tidak ada korelasi
    + mendekati 180 derajat = korelasi negatif
    
* Letak titik observasi terhadap arah panah:

  - Observasi yang searah panah dan letaknya jauh dari pusat data (titik nol) maka memiliki nilai yang besar di variable panah tersebut
  - Observasi yang berlawanan arah panah, maka nilainya kecil di variable tersebut
  - Observasi dekat dengan titik nol, maka nilainya mendekati nilai rata-rata variable tersebut

<center>**<----------- END OF DAY 2 ----------->**</center>

### Dimension Description

Untuk lebih jelas dan objektif, kontribusi/korelasi tiap variable ke tiap PC dapat dilihat menggunakan `dimdesc()`. Semakin tinggi nilai korelasi, semakin banyak informasi yang dirangkum pada PC tersebut.

```{r}
# dimdisc: dimension description
loan_dim <- dimdesc(loan_pca)

# variable yang berkontribusi untuk PC1
as.data.frame(loan_dim$Dim.1$quanti)
```

💡 Mirip dengan `fviz_contrib()` yang kita pelajari sebelumnya

```{r}
fviz_contrib(X = loan_pca,
             choice = "var",
             axes = 1)
```

⚠️ Besar kontribusi tidak dilihat dari tanda negatif/positif pada korelasi, melainkan hanya magnitudenya (nilai absolutenya)

### Reduce Dimension

Menampilkan cumulative proportion dari PCA:

```{r}
# Summary PCA untuk FactoMineR
loan_pca$eig
```

Misalkan kita ingin mempertahankan informasi sebesar 80%, maka berapa PC yang kita gunakan? **PC 1 - 5**

```{r}
head(loan_pca$ind$coord[,1:5])
```

```{r}
# mengambil data hasil PCA sebanyak PC yang dibutuhkan
loan_keep <- as.data.frame(loan_pca$ind$coord[,1:5])
head(loan_keep)
```

`loan_keep` kemudian dapat dikembalikan ke data frame awal (menggantikan variable numerik awal) dan digunakan untuk supervised learning.

## [Additional] Reconstruction

Fungsi `reconst()` memungkinkan kita untuk melakukan rekonstruksi data hasil PCA ke data semula. Umumnya rekonstruksi ini digunakan pada data gambar untuk kasus image compression. 

Misalnya gambar asli adalah 40x40 pixel (1600 kolom), lalu direduksi dengan menggunakan 50 PC saja. Untuk menampilkan data berupa gambar, tetap diperlukan struktur data yang 1600 kolom yang mewakili tiap pixel. Kita bisa melakukan rekonstruksi gambar dari 50 PC ke 1600 kolom kembali. Namun, nilai hasil rekonstruksi tersebut menjadi sedikit berbeda dari nilai awal.

**Catatan**: rekonstruksi data jarang dilakukan pada data tabular, karena nilai hasil rekonstruksi tidak merepresentasikan nilai asli, sehingga kurang ada manfaatnya.

Berikut contoh rekonstruksi data dengan menggunakan data `loan`: 

```{r}
# reconstruct data menggunakan PC1 - PC5
loan_reconst <- reconst(loan_pca, 
                        ncp = 5) # jumlah PC yang ingin di reconstruct

head(data.frame(loan_reconst), 3)
```

Bandingkan dengan nilai asli sebelum dilakukan PCA:

```{r}
# data awal (coba bandingkan dengan hasil reconstruct di atas)
loan_clean %>% 
  select_if(is.numeric) %>% 
  head(3)
```

Kita juga dapat rekonstruksi data hingga 100% sama persis bila menggunakan seluruh PC, dengan menggunakan parameter `ncp = 8` di fungsi `reconst()`.

## [Optional] Eigenfaces

**Bagian berikut menyajikan contoh rekonstruksi PCA pada data gambar.**

Pada kasus face recognition, data gambar yang memiliki banyak sekali dimensi dapat mengalami apa yang disebut sebagai **the curse of dimensionality**. Setelah melewati batas tertentu, kemampuan suatu model dalam memprediksi hasil akan semakin menurun seiring dengan bertambahnya dimensi.

Dengan melakukan dimensionality reduction (PCA) pada data gambar dan merekonstruksi data gambar dari data hasil PCA, kita dapat meningkatkan performa model face recognition kita.

```{r}
# read `faceData` 32 pixel
load("data_input/face.rda")

# PCA
face_pca <- PCA(faceData, graph = F, ncp = 32)
# face_pca$eig
```

```{r}
# create reconstructed data
face_recon <- reconst(face_pca, ncp = 2)
face_recon2 <- reconst(face_pca, ncp = 4)
face_recon3 <- reconst(face_pca, ncp = 6)
face_recon4 <- reconst(face_pca, ncp = 32) # mengambil semua PC
```

```{r fig.height= 8, fig.width=8}
# make function to visualize image data
showMatrix <- function(x, title){
  image(t(x[nrow(x):1,]), 
        xaxt = 'n', yaxt = 'n', 
        col = gray((0:32)/32),
        main = title, 
        font.main=4, 
        cex.main=0.5
        )
}

# visualize image data
par(mfrow=c(2,2), mar=c(0.5,0.5,1.5,0.5))
showMatrix(faceData, title = 'Original Image')
showMatrix(face_recon, title = 'Reconstructed: 2 dimensions')
showMatrix(face_recon3, title = 'Reconstructed: 6 dimensions')
showMatrix(face_recon4, title = 'Reconstructed: All PCs')
```

## 📝 SUMMARY FUNGSI-FUNGSI

- `biplot(prcomp())`: membuat biplot dengan base R
- `fviz_contrib()`: melihat urutan kontribusi tiap variable ke tiap PC, dari package `factoextra`
- `PCA()`: membuat objek PCA dari package `FactoMineR`
- `plot.PCA()`: membuat biplot dari object `PCA()`:
  + `choix = "ind"`: visualisasi sebaran data (individual factor map), terdapat parameter `habillage` untuk mewarnai observasi berdasarkan index kolom
  + `choix = "var"`: visualisasi variable factor map (panah)
- `dimdesc()`: dimension description, untuk melihat kontribusi variable ke tiap PC
- `reconst()`: melakukan rekonstruksi dari nilai PC ke data asli, umumnya untuk data gambar

***

# Clustering

Clustering adalah pengelompokan data berdasarkan karakteristiknya. Clustering bertujuan untuk menghasilkan cluster dimana:

- observasi di satu cluster yang sama memiliki karakteristik semirip mungkin
- observasi dari cluster berbeda memiliki karakteristik yang sebeda mungkin

## K-means Clustering

K-means adalah salah satu algoritma clustering yang *centroid-based*, artinya tiap cluster memiliki satu centroid yang mewakili cluster tersebut.

💡 Banyaknya cluster $k$ ditentukan oleh user.

```{r, out.width = "70%", echo = FALSE, fig.align = "center"}
knitr::include_graphics("additional_assets/kmeans.png")
```

### Algoritma

1. **Random initialization**: meletakkan $k$ centroid secara random
2. **Cluster assignment**: assign masing-masing observasi ke cluster terdekat, berdasarkan perhitungan jarak (jarak Euclidean)
3. **Centroid update**: menggeser centroid ke rata-rata (means) dari cluster yang terbentuk
4. Ulangi langkah 2 dan 3 sampai tidak ada observasi yang clusternya berubah lagi

```{r message=FALSE, warning=FALSE, eval=FALSE}
# pakai 'interval' yang lebih tinggi bila animasi terlalu cepat
# jalankan command ini di console:

library(animation)
RNGkind(sample.kind = "Rounding")
set.seed(100)
ani.options(interval = 2)
par(mar = c(3, 3, 1, 1.5), mgp = c(1.5, 0.5, 0))
kmeans.ani()
```

💡 Kunjungi link berikut untuk visualisasi yang interaktif: https://www.naftaliharris.com/blog/visualizing-k-means-clustering

### Workflow K-means Clustering 

#### Business Question: Whisky Recommendation

Kita sebagai seorang data scientist sebuah toko whisky diminta untuk membuat product recommendation untuk whisky berdasarkan preferensi rasa masing-masing customer!

Tujuan: membentuk kelompok whisky yang memiliki karakteristik rasa khas pada tiap clusternya

#### Read Data

Data yang digunakan berupa data penyulingan Malt Whisky dari 86 pabrik penyulingan, diperoleh dari penelitian Dr. Wisehart (Universitas St. Andrews). Setiap whisky diberi skor 0-4 dari 12 kategori cita rasa berdasarkan uji organoleptik:

* Body: tingkat kekuatan rasa (light/heavy)
* Sweetness: tingkat rasa manis
* Smoky: tingkat rasa asap
* Medicinal: tingkat rasa pahit (obat)
* Tobacco: tingkat rasa tembakau
* Honey: tingkat rasa madu
* Spicy: tingkat rasa pedas
* Winey: tingkat rasa anggur
* Nutty: tingkat rasa kacang
* Malty: tingkat rasa gandum
* Fruity: tingkat rasa buah
* Floral: tingkat rasa bunga

```{r}
whisky <- read.csv("data_input/whiskies.txt")
head(whisky)
```

#### Data Cleansing

```{r}
# meng-assign nilai dari kolom Distillery menjadi rownames
rownames(whisky) <- whisky$Distillery

# membuang kolom yang tidak digunakan
whisky <- whisky %>% 
  select(-c(RowID, Distillery, Postcode, Latitude, Longitude))

head(whisky)
```

#### EDA

Cek missing values:

```{r}
colSums(is.na(whisky))
```

> Tidak ada missing values

Cek skala antar variabel

```{r}
summary(whisky)
```

Diskusi: Pada data `whisky`, apakah skala nilai antar variable berbeda? Apakah perlu dilakukan scaling?

Tidak perlu melakukan scaling karena dari deskripsi datapun rangenya sudah dari 0-4

#### K-means

Parameter:

- `x`: dataset
- `centers`: banyaknya centroid $k$

Note: perlu dilakukan `set.seed()` karena terdapat random initialization pada tahap awal k-means

```{r}
# k-means dengan 3 cluster
RNGkind(sample.kind = "Rounding")
set.seed(100)

whisky_km <- kmeans(whisky, centers = 3)
```

Hasil dari `kmeans()`:

1. Banyaknya pengulangan (iterasi) algoritma k-means sampai dihasilkan cluster yang stabil

```{r}
# jarang dilihat
whisky_km$iter
```

2. Banyaknya observasi pada tiap cluster

```{r}
whisky_km$size
```

3. Letak pusat cluster/centroid, biasa digunakan untuk profiling cluster

```{r}
whisky_km$centers
```

4. Label cluster untuk tiap observasi

```{r}
head(whisky_km$cluster)
```

#### Goodness of Fit

Kebaikan hasil clustering dapat dilihat dari 3 nilai:

- Within Sum of Squares (`$withinss`): jumlah jarak kuadrat dari tiap observasi ke centroid tiap cluster.
- Between Sum of Squares (`$betweenss`): jumlah jarak kuadrat terbobot dari tiap centroid ke rata-rata global. Dibobotkan berdasarkan banyaknya observasi pada cluster.
- Total Sum of Squares (`$totss`): jumlah jarak kuadrat dari tiap observasi ke rata-rata global.

```{r}
# cek nilai WSS dan rasio BSS/TSS
sum(whisky_km$withinss) # ekuivalen dengan $tot.withinss
whisky_km$tot.withinss # WSS

whisky_km$betweenss / whisky_km$totss # rasio BSS/TSS
```

Clustering yang "baik":

- WSS semakin rendah: jarak observasi di 1 kelompok yang sama semakin rendah, artinya tiap cluster memiliki karakteristik yang semakin mirip
- $\frac{BSS}{TSS}$ mendekati 1, karena kelompok hasil clustering semakin mewakili persebaran data yang sesungguhnya

Diskusi: Coba buatlah k-means dengan $k$ sebesar mungkin, misalnya 80. Kemudian cek WSS dan rasio BSS/TSSnya, apakah clustering yang terbentuk dapat dikatakan ideal?

```{r}
RNGkind(sample.kind = "Rounding")
set.seed(100)

# k-means clustering
whisky_km_80 <- kmeans(whisky, centers=80)

# cek nilai WSS dan rasio BSS/TSS
whisky_km_80$tot.withinss
whisky_km_80$betweenss / whisky_km_80$totss
```

- $k = 3$

  - 447.197 -> WSS
  - 0.3283688 -> rasio BSS/TSS

- $k = 80$

  - 6.5 -> WSS
  - 0.9902379 -> rasio BSS/TSS

Jawaban: WSS semakin mendekati 0, BSS/TSS semakin mendekati 1, namun cluster dengan k = 80 tidak berguna karena tiap cluster observasinya hanya 1-2

#### Pemilihan k optimum

Semakin tinggi $k$: 

- WSS semakin mendekati 0
- BSS semakin mendekati TSS (atau BSS/TSS mendekati 1)

Kalau begitu apakah kita selalu memilih $k$ = banyak observasi? Bagaimana menentukan $k$ optimum?

1. Kebutuhan dari segi bisnis, dibutuhkan menjadi berapa kelompok
2. Secara objektif: Elbow method, visualisasi dengan `fviz_nbclust()`

```{r}
library(factoextra)
fviz_nbclust(whisky, FUNcluster = kmeans, method = "wss")
```

Pilih nilai $k$ dimana ketika $k$ ditambah, penurunan Total WSS tidak terlalu drastis (atau dapat dikatakan melandai).

Buat ulang k-means dengan $k$ optimum:

```{r}
RNGkind(sample.kind = "Rounding")
set.seed(100)

# berdasarkan elbow method, k = 5 (subjektif)
whisky_km_5 <- kmeans(whisky, centers = 5)
```

#### Interpretasi/Cluster Profiling

```{r message=FALSE, warning=FALSE}
# memasukkan label cluster ke data awal
whisky$kelompok <- whisky_km_5$cluster

# melakukan profiling dengan summarise data
whisky %>% 
  group_by(kelompok) %>% 
  summarise_all(mean)
```

Profiling:

- Cluster 1: honey, winey, nutty, malty, fruity paling tinggi diantara kelompok lainnya -> whisky manis nano-nano
- Cluster 2: body, smoky, medicinal, tobacco paling tinggi -> whisky pahit 
- Cluster 3: body paling rendah, floral paling tinggi -> whisky manis aroma bunga
- Cluster 4: tidak ada yang mendominasi < 2 -> whisky balance/light
- Cluster 5: sweet, spicy paling tinggi -> whisky manis pedas

#### Product Recommender

Misal ada 1 pelanggan yang menyukai whisky **Laphroig** datang ke toko kita, namun stok whisky tersebut sedang kosong. Kira-kira whisky apa yang akan kita rekomendasikan?

```{r}
# cek Laphroig ada di kelompok mana?
whisky["Laphroig", ] # kelompok 2

# cek whisky apa saja yang ada di kelompok 2
whisky %>% 
  filter(kelompok == 2)
```

Jawaban: Laphroig berada di cluster 2, whisky yang pahit. Whisky lain yang memiliki cita rasa yang sama adalah: Ardbeg, Caol Ila, Clynelish, Lagavulin, Talisker

### 📝 SUMMARY DAY 3

**Clustering**

Mengelompokkan data berdasarkan kemiripan karakteristik, harapannya:

- observasi pada satu cluster yang sama semakin mirip
- observasi dari cluster yang berbeda semakin berbeda/jauh

**K-means Clustering**

- Mengelompokkan data menjadi $k$ cluster yang ditentukan oleh user
- Pengelompokan berdasarkan jarak Euclidean antara centroid (titik pusat) cluster dengan tiap observasi.
- Kriteria clustering yang "baik":
  + Within sum of squares mendekati nilai 0 (sekecil mungkin)
  + Rasio Between sum of squares dengan Total sum of squares mendekati nilai 1
- Memilih $k$ optimum:
  + Kebutuhan bisnis
  + Elbow method
  
**Workflow K-means**

1. Business question
2. Read data
3. Data cleansing
4. EDA: cek skala dan tipe data harus numerik
5. Data Preprocessing: scaling apabila skala berbeda
6. Menentukan $k$ optimum
7. Membuat cluster dengan fungsi `kmeans(data, center = k)`
8. Kolom label hasil clustering digabungkan dengan data awal
9. Profiling cluster untuk memahami karakteristik tiap cluster, dapat digunakan untuk rekomendasi produk, dll
10. Tambahan: clustering dapat divisualisasikan dengan biplot untuk mempermudah profiling

<center>**<----------- END OF DAY 3 ----------->**</center>

### [Additional] Visualize Clustering (Bar plot)

Untuk mempermudah cluster profiling, kita dapat melakukan visualisasi bar plot seperti berikut:

```{r}
library(ggplot2)
library(reshape2)

# nilai centroid
whisky_profile <- whisky %>% 
  mutate(kelompok = as.factor(kelompok)) %>% 
  group_by(kelompok) %>% 
  summarise_all(mean)

# reshaping data
whisky_melt <- melt(whisky_profile, id.vars = "kelompok")

# visualisasi data
ggplot(whisky_melt, 
       aes(x = kelompok, y = value, fill = kelompok)) + 
  geom_col() + 
  facet_wrap(~variable)
```

> Credits to Pak Bayu Andrianto

### Visualize clustering (PCA Biplot + Kmeans)

Misalkan ada kebutuhan untuk memvisualisasikan hasil clustering tersebut. Namun pada dataset `whisky` terdapat 12 variabel numerik, bagaimana cara kita memvisualisasikannya pada plot 2 dimensi?

```{r}
# gunakan fungsi fviz_cluster() dari package factoextra
fviz_cluster(object = whisky_km_5, # object kmeans
             data = whisky %>% select(-kelompok)) # data variable numerik

```

Misal kita ingin buat sebuah visualisasi yang mempermudah cluster profiling, dimana tampil gabungan individual + variable factor map menjadi satu. Visualisasi dapat dibuat menggunakan fungsi `fviz_pca_biplot()` dari package `factoextra`

```{r}
# PCA menggunakan FactoMineR
whisky_pca <- PCA(X=whisky,
                  scale.unit = F, # default T , kita tidak ingin membuat scale
                  quali.sup = 13, # kolom kelompok / kategoru
                  graph = F) # biar tidak langsung nge plot
whisky_pca$eig
```

```{r}
# visualisasi PCA + hasil kmeans clustering
fviz_pca_biplot(X=whisky_pca,
                habillage = 13,
                geom.ind = "point",
                addEllipses = T)
```

# Further Reading

- [Askalgo: Unsupervised Learning](https://askalgo.netlify.app/#unsupervised-learning)
- [Medium: Image Compression using K-means and PCA](https://medium.com/analytics-vidhya/image-compression-using-k-means-clustering-and-principal-component-analysis-in-python-893fd37e8e15)

## Dimensionality Reduction

- [Askalgo: Time Efficiency and Accuracy Improvement using PCA](https://algotech.netlify.app/blog/time-and-accuracy-improvement-using-pca/)

## Clustering

- [Algotech: Clustering Saham (K-means dan K-medoid)](https://algotech.netlify.app/blog/stock-cluster/)
- [Algotech: Hierarchical Clustering](https://algotech.netlify.app/blog/introduction-to-hierarchical-clustering/)
- [Algotech: DBSCAN Clustering](https://algotech.netlify.app/blog/dbscan-clustering/)
- [Algotech: Fuzzy C-means Clustering](https://algotech.netlify.app/blog/fuzzy-clustering/)